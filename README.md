# BLIP-2 â€” Bootstrapping Languageâ€“Image Pretraining with Frozen Image Encoders and LLMs

> **Course:** DS 5690 â€” Gen AI Models in Theory & Practice (2025F)  
> **Presenter:** *(your name)*  
> **Paper:** Li et al., 2023 â€” â€œBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Modelsâ€ (arXiv:2301.12597)

---

## TL;DR
BLIPâ€‘2 makes multimodal training **much cheaper** by keeping the **image encoder** and the **LLM** **frozen**, and learning only a lightweight **Querying Transformer (Qâ€‘Former)** to bridge images â†’ language. It pretrains Qâ€‘Former in **two stages**: (1) representation learning with an image encoder (ITC/ITM/ITG), and (2) generative learning by prompting a frozen LLM with projected visual queries. Despite far fewer *trainable* parameters, BLIPâ€‘2 matches or beats larger endâ€‘toâ€‘end models on zeroâ€‘shot VQA, captioning, and retrieval.

---

## Fiveâ€‘Minute Overview (Context â†’ Problem â†’ Approach â†’ Results)
- **Context.** Visionâ€“language pretraining (VLP) models got huge and expensive; endâ€‘toâ€‘end training is prohibitive.  
- **Problem.** How to achieve strong multimodal performance **without** full endâ€‘toâ€‘end training?  
- **Approach.** Freeze a **strong image encoder** (e.g., CLIP ViT) and a **strong LLM** (e.g., Flanâ€‘T5/OPT), and train a small **Qâ€‘Former** that extracts a compact set of **learned visual queries** and **prompts** the LLM.  
- **Results.** Stateâ€‘ofâ€‘theâ€‘art **zeroâ€‘shot** VQA, strong captioning and retrievalâ€”while using vastly fewer *trainable* params than e2e models.

---

## Architecture (Highâ€‘Level)


![BLIP-2 Overview](./images/blip2.png)



** Method
- The BLIP-2 framework, short for Bootstrapping Language-Image Pre-training 2, introduces an efficient approach for aligning vision and language without the need for end-to-end training of massive multimodal models. Instead of jointly training an image encoder and a language model from scratch, BLIP-2 leverages two powerful pre-trained unimodal components: a frozen image encoder (such as CLIP ViT-L/14 or EVA-CLIP ViT-g/14) and a frozen large language model (LLM) such as OPT or FlanT5. Between these two frozen modules lies the only trainable component â€” the Querying Transformer (Q-Former) â€” which serves as a lightweight bridge that learns how to translate visual representations into language-understandable embeddings.

The training of BLIP-2 proceeds in two stages, each targeting a distinct aspect of cross-modal alignment.

Stage One: Visionâ€“Language Representation Learning.
In this stage, the model aims to teach Q-Former how to extract the visual information that is most relevant to textual semantics. Given a pair of image and caption, Q-Former interacts with the frozen image encoder through cross-attention layers using a fixed set of learnable query tokens. It is optimized jointly by three complementary objectives:
(1) Image-Text Contrastive Learning (ITC) to align image and text embeddings in a shared latent space;
(2) Image-Grounded Text Generation (ITG) to generate captions conditioned on visual features; and
(3) Image-Text Matching (ITM) to discriminate whether an imageâ€“text pair is correctly matched.
By combining these objectives, Q-Former gradually learns to focus on semantically meaningful regions of the image while filtering out irrelevant visual details.

Stage Two: Vision-to-Language Generative Learning.
After Q-Former has learned to represent images in a language-related manner, the second stage connects it to a frozen LLM to endow the whole system with natural-language generation ability. The key idea is to project the output of Q-Former â€” a set of 32 visual query embeddings â€” into the same dimensional space as the LLMâ€™s word embeddings, and then prepend these visual embeddings to the text input sequence. They act as soft visual prompts that condition the LLM on the image content.

To illustrate, consider an input image showing a cat wearing sunglasses.

The frozen image encoder first extracts dense visual features.

Q-Former compresses these features into 32 informative queries that summarize â€œwhat the image is about.â€

These queries are linearly mapped to the token space and inserted before the text prompt fed into the LLM, for example:
â€œ
ğ‘£
ğ‘–
ğ‘ 
ğ‘¢
ğ‘
ğ‘™
ğ‘
ğ‘Ÿ
ğ‘œ
ğ‘š
ğ‘
ğ‘¡
ğ‘ 
visualprompts A cat wearing sunglassesâ€.

The LLM (e.g., OPT or FlanT5) then generates the natural language output such as â€œA cat wearing sunglasses sitting on a beach.â€
During training, the system minimizes the standard language-modeling loss so that the generated text aligns with ground-truth captions or answers. In essence, the second stage teaches Q-Former to speak the LLMâ€™s language â€” to express visual information in a form that the LLM can interpret.

This two-stage strategy offers several advantages. Because both the image encoder and the LLM remain frozen, the pre-training cost is drastically reduced. Moreover, it avoids catastrophic forgetting in the LLM while still achieving strong visionâ€“language alignment. Through this process, BLIP-2 can perform a wide range of zero-shot multimodal tasks â€” from image captioning and visual question answering to instruction-based image-to-text generation â€” demonstrating that the lightweight Q-Former is sufficient to bridge the gap between powerful unimodal models.
- **Crossâ€‘attends** to frozen ViT features to pull out languageâ€‘relevant information.  
- Feeds projected queries as **soft visual prompts** to the frozen LLM.

---

## Twoâ€‘Stage Pretraining

### Stage 1 â€” *Visionâ€“Language Representation Learning* (with frozen image encoder)
Jointly optimize three objectives to make queries languageâ€‘relevant:  
- **ITC** (Imageâ€“Text Contrastive): align global image/text embeddings.  
- **ITM** (Imageâ€“Text Matching): binary â€œmatch?â€ classification with hard negatives.  
- **ITG** (Imageâ€‘Grounded Generation): force queries to contain all info needed to generate text.

### Stage 2 â€” *Visionâ†’Language Generative Learning* (with frozen LLM)
- Project query features to the LLMâ€™s token dim and **prepend** them to the text tokens (soft prompts).  
- Train only **Qâ€‘Former + projection**, keeping LLM **frozen**, to enable captioning/QA conditioned on visual prompts.

---

## Formal Pseudocode

```python
# Notation:
#   E_img: frozen image encoder (ViT)
#   Q: Qâ€‘Former (trainable)
#   P: linear projection from Qâ€‘Former output to LLM token dim (trainable)
#   LLM: frozen large language model (decoder or encoderâ€‘decoder)
#   ITC, ITM, ITG: stageâ€‘1 losses; LM_loss / PrefixLM_loss: stageâ€‘2 losses

# ---------- Stage 1: Representation Learning ----------
for image, text in D_image_text:
    V = E_img(image)                  # frozen features
    Z = Q(image_feats=V, text=None)   # queries attend to V via crossâ€‘attention
    loss = ITC(Z, text) + ITM(Z, text) + ITG(Z, text)
    update(Q)                         # only Qâ€‘Former is updated

# ---------- Stage 2: Generative Learning ----------
for image, text in D_image_text:
    V = E_img(image)                  # frozen features
    Z = Q(image_feats=V, text=None)   # extract visual queries (languageâ€‘relevant)
    V_prompt = P(Z)                   # project to LLM embedding size
    loss = LM_loss(LLM(prompt=V_prompt, text=text))  # or PrefixLM for encoderâ€‘decoder
    update(Q, P)                      # only Qâ€‘Former + projection are updated
```

---

## Critical Analysis (Whatâ€™s strong / Whatâ€™s missing)
**Strengths**
- Large *frozen* components preserve pretrained knowledge; few trainable params â†’ **computeâ€‘efficient**.
- Twoâ€‘stage scheme reduces **catastrophic forgetting** and improves zeroâ€‘shot performance.
- Modular: can â€œharvestâ€ better ViTs/LLMs over time.

**Limitations / Open Questions**
- Singleâ€‘pair pretraining lacks multiâ€‘image interleaving â†’ weak **inâ€‘context** multimodal examples; limited fewâ€‘shot gains.  
- Quality still bounded by the LLMâ€™s knowledge (bias, hallucination).  
- Visual reasoning can fail on novel or complex scenes; struggles with very long visual contexts.

---

## Impact
- Helped establish the **â€œfrozen LLM + visual adapterâ€** recipe used by **LLaVA**, **InstructBLIP**, **MiniGPTâ€‘4**, etc.  
- Lowered the barrier to building visual assistants on modest compute while staying competitive with very large e2e models.

---

## Demo (Captioning + VQA)
Use the notebook [`demo.ipynb`](./demo.ipynb). It loads `Salesforce/blip2-flan-t5-xl` from ğŸ¤— Transformers, captions an image, and answers a visual question.

**Environment (suggested)**
```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # or CPU wheels
pip install -U transformers accelerate pillow safetensors
```

**Run**
1. Open the notebook, set `IMAGE_PATH` to a local file (or URL).  
2. Run the caption cell.  
3. Set a `question` string and run the VQA cell.

---

## Resource Links
1. Paper: https://arxiv.org/abs/2301.12597  
2. BLIPâ€‘2 in LAVIS (Salesforce): https://github.com/salesforce/LAVIS/tree/main/projects/blip2  
3. ğŸ¤— Model Card (Flanâ€‘T5 XL): https://huggingface.co/Salesforce/blip2-flan-t5-xl  
4. ğŸ¤— Model Card (OPT 2.7B): https://huggingface.co/Salesforce/blip2-opt-2.7b  
5. Colabâ€‘style starter (community): https://colab.research.google.com/github/salesforce/LAVIS/blob/main/docs/source/tutorials/BLIP2_captioning.ipynb

---

## Citation
```bibtex
@article{li2023blip2,
  title   = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author  = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal = {arXiv preprint arXiv:2301.12597},
  year    = {2023}
}
```

---

## Appendix: Notes for Presentation (Rubricâ€‘friendly)
- **Overview:** keep to 5 min; state problem clearly (cost), highlight twoâ€‘stage idea & wins.  
- **Architecture:** paste the pseudocode; keep diagram simple; emphasize frozen ViT/LLM and trainable Qâ€‘Former.  
- **Critical Analysis:** cover inâ€‘context limitation + reliance on LLM knowledge.  
- **Impacts:** mention how it shaped later visual chatbots.  
- **Two audience questions:**  
  1) *Why freeze the LLM instead of finetuning it endâ€‘toâ€‘end?*  
  2) *How does Stageâ€‘1 representation learning prevent catastrophic forgetting in Stageâ€‘2?*
