# BLIP-2 â€” Bootstrapping Languageâ€“Image Pretraining with Frozen Image Encoders and LLMs

> **Course:** DS 5690 â€” Gen AI Models in Theory & Practice (2025F)  
> **Presenter:** *(your name)*  
> **Paper:** Li et al., 2023 â€” â€œBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Modelsâ€ (arXiv:2301.12597)

---

## TL;DR
BLIPâ€‘2 makes multimodal training **much cheaper** by keeping the **image encoder** and the **LLM** **frozen**, and learning only a lightweight **Querying Transformer (Qâ€‘Former)** to bridge images â†’ language. It pretrains Qâ€‘Former in **two stages**: (1) representation learning with an image encoder (ITC/ITM/ITG), and (2) generative learning by prompting a frozen LLM with projected visual queries. Despite far fewer *trainable* parameters, BLIPâ€‘2 matches or beats larger endâ€‘toâ€‘end models on zeroâ€‘shot VQA, captioning, and retrieval.

---

## Fiveâ€‘Minute Overview (Context â†’ Problem â†’ Approach â†’ Results)
- **Context.** Visionâ€“language pretraining (VLP) models got huge and expensive; endâ€‘toâ€‘end training is prohibitive.  
- **Problem.** How to achieve strong multimodal performance **without** full endâ€‘toâ€‘end training?  
- **Approach.** Freeze a **strong image encoder** (e.g., CLIP ViT) and a **strong LLM** (e.g., Flanâ€‘T5/OPT), and train a small **Qâ€‘Former** that extracts a compact set of **learned visual queries** and **prompts** the LLM.  
- **Results.** Stateâ€‘ofâ€‘theâ€‘art **zeroâ€‘shot** VQA, strong captioning and retrievalâ€”while using vastly fewer *trainable* params than e2e models.

---

## Architecture (Highâ€‘Level)

```
Image            Qâ€‘Former (trainable)                     Frozen LLM
Encoder          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Linear proj.        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
(ViT)  â”€â”€â”€â”€â”€â”€â”€â–º  â”‚ 32 learned    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º       â”‚ Text gen. â”‚
features         â”‚ query tokens  â”‚                      â”‚ or QA     â”‚
                 â”‚ (selfâ€‘attn)   â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ + crossâ€‘attn  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–²
                 Frozen image features
```

**Key piece: Qâ€‘Former**  
- Maintains a small, fixed number of trainable **query tokens** (e.g., 32Ã—768).  
- **Crossâ€‘attends** to frozen ViT features to pull out languageâ€‘relevant information.  
- Feeds projected queries as **soft visual prompts** to the frozen LLM.

---

## Twoâ€‘Stage Pretraining

### Stage 1 â€” *Visionâ€“Language Representation Learning* (with frozen image encoder)
Jointly optimize three objectives to make queries languageâ€‘relevant:  
- **ITC** (Imageâ€“Text Contrastive): align global image/text embeddings.  
- **ITM** (Imageâ€“Text Matching): binary â€œmatch?â€ classification with hard negatives.  
- **ITG** (Imageâ€‘Grounded Generation): force queries to contain all info needed to generate text.

### Stage 2 â€” *Visionâ†’Language Generative Learning* (with frozen LLM)
- Project query features to the LLMâ€™s token dim and **prepend** them to the text tokens (soft prompts).  
- Train only **Qâ€‘Former + projection**, keeping LLM **frozen**, to enable captioning/QA conditioned on visual prompts.

---

## Formal Pseudocode

```python
# Notation:
#   E_img: frozen image encoder (ViT)
#   Q: Qâ€‘Former (trainable)
#   P: linear projection from Qâ€‘Former output to LLM token dim (trainable)
#   LLM: frozen large language model (decoder or encoderâ€‘decoder)
#   ITC, ITM, ITG: stageâ€‘1 losses; LM_loss / PrefixLM_loss: stageâ€‘2 losses

# ---------- Stage 1: Representation Learning ----------
for image, text in D_image_text:
    V = E_img(image)                  # frozen features
    Z = Q(image_feats=V, text=None)   # queries attend to V via crossâ€‘attention
    loss = ITC(Z, text) + ITM(Z, text) + ITG(Z, text)
    update(Q)                         # only Qâ€‘Former is updated

# ---------- Stage 2: Generative Learning ----------
for image, text in D_image_text:
    V = E_img(image)                  # frozen features
    Z = Q(image_feats=V, text=None)   # extract visual queries (languageâ€‘relevant)
    V_prompt = P(Z)                   # project to LLM embedding size
    loss = LM_loss(LLM(prompt=V_prompt, text=text))  # or PrefixLM for encoderâ€‘decoder
    update(Q, P)                      # only Qâ€‘Former + projection are updated
```

---

## Critical Analysis (Whatâ€™s strong / Whatâ€™s missing)
**Strengths**
- Large *frozen* components preserve pretrained knowledge; few trainable params â†’ **computeâ€‘efficient**.
- Twoâ€‘stage scheme reduces **catastrophic forgetting** and improves zeroâ€‘shot performance.
- Modular: can â€œharvestâ€ better ViTs/LLMs over time.

**Limitations / Open Questions**
- Singleâ€‘pair pretraining lacks multiâ€‘image interleaving â†’ weak **inâ€‘context** multimodal examples; limited fewâ€‘shot gains.  
- Quality still bounded by the LLMâ€™s knowledge (bias, hallucination).  
- Visual reasoning can fail on novel or complex scenes; struggles with very long visual contexts.

---

## Impact
- Helped establish the **â€œfrozen LLM + visual adapterâ€** recipe used by **LLaVA**, **InstructBLIP**, **MiniGPTâ€‘4**, etc.  
- Lowered the barrier to building visual assistants on modest compute while staying competitive with very large e2e models.

---

## Demo (Captioning + VQA)
Use the notebook [`demo.ipynb`](./demo.ipynb). It loads `Salesforce/blip2-flan-t5-xl` from ğŸ¤— Transformers, captions an image, and answers a visual question.

**Environment (suggested)**
```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # or CPU wheels
pip install -U transformers accelerate pillow safetensors
```

**Run**
1. Open the notebook, set `IMAGE_PATH` to a local file (or URL).  
2. Run the caption cell.  
3. Set a `question` string and run the VQA cell.

---

## Resource Links
1. Paper: https://arxiv.org/abs/2301.12597  
2. BLIPâ€‘2 in LAVIS (Salesforce): https://github.com/salesforce/LAVIS/tree/main/projects/blip2  
3. ğŸ¤— Model Card (Flanâ€‘T5 XL): https://huggingface.co/Salesforce/blip2-flan-t5-xl  
4. ğŸ¤— Model Card (OPT 2.7B): https://huggingface.co/Salesforce/blip2-opt-2.7b  
5. Colabâ€‘style starter (community): https://colab.research.google.com/github/salesforce/LAVIS/blob/main/docs/source/tutorials/BLIP2_captioning.ipynb

---

## Citation
```bibtex
@article{li2023blip2,
  title   = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author  = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal = {arXiv preprint arXiv:2301.12597},
  year    = {2023}
}
```

---

## Appendix: Notes for Presentation (Rubricâ€‘friendly)
- **Overview:** keep to 5 min; state problem clearly (cost), highlight twoâ€‘stage idea & wins.  
- **Architecture:** paste the pseudocode; keep diagram simple; emphasize frozen ViT/LLM and trainable Qâ€‘Former.  
- **Critical Analysis:** cover inâ€‘context limitation + reliance on LLM knowledge.  
- **Impacts:** mention how it shaped later visual chatbots.  
- **Two audience questions:**  
  1) *Why freeze the LLM instead of finetuning it endâ€‘toâ€‘end?*  
  2) *How does Stageâ€‘1 representation learning prevent catastrophic forgetting in Stageâ€‘2?*
